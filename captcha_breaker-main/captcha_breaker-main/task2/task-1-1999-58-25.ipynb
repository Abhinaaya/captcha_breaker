{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10592609,"sourceType":"datasetVersion","datasetId":6556002}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom PIL import Image\nimport os\nimport random\n\nclass dataset(Dataset):\n    def __init__(self, data_dir, transform):\n        self.data_dir = data_dir\n        self.image_files = [file for file in os.listdir(self.data_dir) if file.endswith('.png')]\n        self.transform = transform\n        self.char_to_index = {char : (i+1) for i, char in enumerate(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\")}\n        self.index_to_char = {self.char_to_index[char]:char for char in self.char_to_index}\n        self.index_to_char[0] = '<blank>'\n    \n    def __len__(self):\n        return len(self.image_files)\n    \n    def __getitem__(self, idx):\n        img_name = self.image_files[idx]\n        image_path = os.path.join(self.data_dir, img_name)\n        image = Image.open(image_path).convert('L')\n        image =self.transform(image)\n        label = img_name[:-4]\n        label_indices = [self.char_to_index[i] for i in label]\n        return image, torch.tensor(label_indices)\n\ntransform = transforms.Compose([\n    transforms.Resize((32,128)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\n\nclass CRNN(nn.Module):\n    def __init__(self, num_chars):\n        super().__init__()\n        \n        self.cnn = nn.Sequential(\n            nn.Conv2d(1,64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2),\n            \n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),  # 64 --> 32\n            nn.Dropout(0.1),\n            \n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d((2, 1)),  # 32 --> 16 only height\n            nn.Dropout(0.1),\n                \n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d((2, 1)),  # 16 --> 8 only height\n            nn.Dropout(0.1),   \n        )\n        \n        self.lstm1 = nn.LSTM(512 * 2, 256, bidirectional=True, batch_first=True)\n        self.linear1 = nn.Linear(512, 256)\n        self.lstm2 = nn.LSTM(256, 256, bidirectional=True, batch_first=True)\n        self.linear2 = nn.Linear(512, num_chars + 1)\n        \n    def forward(self,x):\n        conv = self.cnn(x)  # [batch, 512, 2, 32]\n        batch, channel, height, width = conv.size()\n        conv = conv.permute(0, 3, 1, 2)  # [batch, width, channel, height]\n        conv = conv.reshape(batch, width, channel * height)  # [batch, width, feature]\n        \n        lstm_out, _ = self.lstm1(conv)  # [batch, width, 512]\n        lstm_out = self.linear1(lstm_out)  # [batch, width, 256]\n        lstm_out, _ = self.lstm2(lstm_out)  # [batch, width, 512]\n        output = self.linear2(lstm_out)  # [batch, width, num_chars + 1]\n        return output\n\ndef decode_predictions(output, index_to_char):\n    if len(output.shape) == 2:\n        output = output.unsqueeze(0)\n    \n    output = nn.functional.log_softmax(output, dim=-1)\n    predictions = torch.argmax(output, dim=-1) \n    \n    batch_results = []\n    for batch_pred in predictions:\n        chars = []\n        prev_char = None\n        \n        for p in batch_pred:\n            char_idx = p.item()\n            if char_idx != 0 and char_idx != prev_char:  \n                if char_idx in index_to_char:\n                    chars.append(index_to_char[char_idx])\n            prev_char = char_idx\n        \n        batch_results.append(''.join(chars))\n    \n    return batch_results[0] if len(batch_results) == 1 else batch_results\n\ndef predict_captcha(model, image_path, transform, device, index_to_char):\n    image = Image.open(image_path).convert('L')\n    image = transform(image)\n    image = image.unsqueeze(0)\n    \n    model.eval()\n    image = image.to(device)\n    \n    with torch.no_grad():\n        output = model(image)\n        predicted_text = decode_predictions(output, index_to_char)\n    \n    model.train()\n    return predicted_text\n\n\ndef train_model(model, train_data, test_data, optimizer, loss_func, device, index_to_char, num_epochs=100):\n    model.train()\n    \n    for epoch in range(num_epochs):\n        total_loss = 0\n        for batch_idx, (images, labels) in enumerate(train_data):\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            batch_size = images.size(0)\n            outputs = model(images)  # [batch, width, num_chars + 1]\n            \n            input_lengths = torch.full(size=(batch_size,), fill_value=outputs.size(1), dtype=torch.long)\n            target_lengths = torch.sum(labels != 0, dim=1)\n            \n            outputs = outputs.log_softmax(2) \n            outputs = outputs.permute(1, 0, 2)\n            \n            optimizer.zero_grad()\n            loss = loss_func(outputs, labels, input_lengths, target_lengths)\n            loss.backward()\n            \n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n            optimizer.step()\n            \n            total_loss += loss.item()\n            \n        model.eval()\n        correct = 0\n        total_samples = len(test_data.dataset)\n        random_index = random.randint(0,total_samples)\n        \n        with torch.no_grad():\n            for i, (test_img, test_label) in enumerate(test_data):\n                test_img = test_img.to(device)\n                predicted_text = decode_predictions(model(test_img), index_to_char)\n                true_text = ''.join([index_to_char[idx.item()] for idx in test_label[0] if idx.item() != 0])\n                \n                if predicted_text == true_text:\n                    correct += 1\n                \n                if i == random_index:\n                    print(f'Sample {i+1}')\n                    print(f'Original text: {true_text}')\n                    print(f'Predicted: {predicted_text}\\n')\n        \n        accuracy = correct / total_samples\n        model.train()\n        print(f'Epoch {epoch+1} complete, Average Loss: {total_loss/len(train_data):.4f}')\n        print(f'Accuracy: {accuracy:.4f}')\n\ndef validate_sample(model, image, label, device, index_to_char):\n    image = image.to(device)\n    output = model(image)\n    predicted_text = decode_predictions(output, index_to_char) \n    true_text = ''.join([index_to_char[idx.item()] for idx in label[0] if idx.item() != 0])\n    return predicted_text, true_text\n\ndef collate_fn(batch):\n    batch.sort(key=lambda x: len(x[1]), reverse=True)\n    images, labels = zip(*batch)\n    images = torch.stack(images, 0)\n    lengths = [len(label) for label in labels]\n    max_length = max(lengths)\n    padded_labels = torch.zeros(len(labels), max_length).long()\n    for i, label in enumerate(labels):\n        padded_labels[i, :len(label)] = label\n        \n    return images, padded_labels\n\ndata = dataset('/kaggle/input/precog-data-2/data', transform)\nnum_chars = len(data.index_to_char) \ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = CRNN(num_chars).to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.003)\n\ntrain_size = int(0.8 * len(data))\ntest_size = len(data) - train_size\ntrain_dataset, test_dataset = random_split(data, [train_size, test_size])\ntrain_data = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\ntest_data = DataLoader(test_dataset, shuffle=True, collate_fn=collate_fn)\nloss_func = nn.CTCLoss(blank=0, zero_infinity=True, reduction='mean')\n\ntrain_model(model, train_data, test_data, optimizer, loss_func, device, data.index_to_char, num_epochs=100)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T10:44:56.868634Z","iopub.execute_input":"2025-02-02T10:44:56.868937Z","iopub.status.idle":"2025-02-02T10:56:24.696017Z","shell.execute_reply.started":"2025-02-02T10:44:56.868909Z","shell.execute_reply":"2025-02-02T10:56:24.695210Z"}},"outputs":[{"name":"stdout","text":"Sample 186\nOriginal text: Wkkwos\nPredicted: \n\nEpoch 1 complete, Average Loss: 4.5734\nAccuracy: 0.0000\nSample 367\nOriginal text: Pynqx\nPredicted: \n\nEpoch 2 complete, Average Loss: 4.2093\nAccuracy: 0.0000\nSample 139\nOriginal text: qHiGtSH\nPredicted: \n\nEpoch 3 complete, Average Loss: 4.1196\nAccuracy: 0.0000\nSample 319\nOriginal text: Eaffnqu\nPredicted: \n\nEpoch 4 complete, Average Loss: 4.0540\nAccuracy: 0.0000\nSample 10\nOriginal text: NDtnVd\nPredicted: \n\nEpoch 5 complete, Average Loss: 3.9816\nAccuracy: 0.0000\nSample 181\nOriginal text: WxFXzME\nPredicted: \n\nEpoch 6 complete, Average Loss: 3.9035\nAccuracy: 0.0000\nSample 370\nOriginal text: oUtQY\nPredicted: \n\nEpoch 7 complete, Average Loss: 3.8670\nAccuracy: 0.0000\nSample 221\nOriginal text: iChFCLO\nPredicted: \n\nEpoch 8 complete, Average Loss: 3.7862\nAccuracy: 0.0000\nSample 335\nOriginal text: YjByJ\nPredicted: \n\nEpoch 9 complete, Average Loss: 3.7097\nAccuracy: 0.0000\nSample 219\nOriginal text: Wwbky\nPredicted: T\n\nEpoch 10 complete, Average Loss: 3.6277\nAccuracy: 0.0000\nSample 221\nOriginal text: Pcmaamu\nPredicted: G\n\nEpoch 11 complete, Average Loss: 3.5823\nAccuracy: 0.0000\nSample 195\nOriginal text: Xiviseb\nPredicted: Ljssh\n\nEpoch 12 complete, Average Loss: 3.5468\nAccuracy: 0.0000\nSample 292\nOriginal text: TwEjM\nPredicted: e\n\nEpoch 13 complete, Average Loss: 3.4537\nAccuracy: 0.0000\nSample 26\nOriginal text: Awowrq\nPredicted: Txcxq\n\nEpoch 14 complete, Average Loss: 3.3454\nAccuracy: 0.0000\nSample 151\nOriginal text: Ddbfmsx\nPredicted: Elkjgee\n\nEpoch 15 complete, Average Loss: 3.2594\nAccuracy: 0.0000\nSample 328\nOriginal text: Mdfdiaj\nPredicted: Cjja\n\nEpoch 16 complete, Average Loss: 3.1124\nAccuracy: 0.0000\nSample 151\nOriginal text: JBwpD\nPredicted: RTwTJ\n\nEpoch 17 complete, Average Loss: 2.9929\nAccuracy: 0.0000\nSample 11\nOriginal text: KwuLRT\nPredicted: wn\n\nEpoch 18 complete, Average Loss: 2.8436\nAccuracy: 0.0000\nSample 98\nOriginal text: Tqxyknm\nPredicted: qxykmn\n\nEpoch 19 complete, Average Loss: 2.7162\nAccuracy: 0.0025\nSample 55\nOriginal text: XNzGd\nPredicted: ROeZZ\n\nEpoch 20 complete, Average Loss: 2.5390\nAccuracy: 0.0025\nSample 238\nOriginal text: Xxaefi\nPredicted: Txaaj\n\nEpoch 21 complete, Average Loss: 2.3457\nAccuracy: 0.0025\nSample 328\nOriginal text: Lrcujb\nPredicted: Kljlb\n\nEpoch 22 complete, Average Loss: 2.2137\nAccuracy: 0.0000\nSample 244\nOriginal text: Fwmfttt\nPredicted: Vwnttt\n\nEpoch 23 complete, Average Loss: 2.1602\nAccuracy: 0.0050\nSample 318\nOriginal text: Ydnhy\nPredicted: YdnhuY\n\nEpoch 24 complete, Average Loss: 1.9792\nAccuracy: 0.0025\nSample 265\nOriginal text: YNhIUr\nPredicted: VThUh\n\nEpoch 25 complete, Average Loss: 1.8439\nAccuracy: 0.0150\nSample 216\nOriginal text: jMTFBpe\nPredicted: uKYRos\n\nEpoch 26 complete, Average Loss: 1.7692\nAccuracy: 0.0050\nSample 61\nOriginal text: Lylv\nPredicted: Rylv\n\nEpoch 27 complete, Average Loss: 1.6823\nAccuracy: 0.0075\nSample 173\nOriginal text: Fwmfttt\nPredicted: Mwmitit\n\nEpoch 28 complete, Average Loss: 1.6664\nAccuracy: 0.0275\nSample 193\nOriginal text: Mqgghv\nPredicted: Eqgghv\n\nEpoch 29 complete, Average Loss: 1.5541\nAccuracy: 0.0050\nSample 72\nOriginal text: Egopron\nPredicted: Egaprgn\n\nEpoch 30 complete, Average Loss: 1.5284\nAccuracy: 0.0150\nSample 179\nOriginal text: Hekjws\nPredicted: Xeklws\n\nEpoch 31 complete, Average Loss: 1.4691\nAccuracy: 0.0050\nSample 57\nOriginal text: lUJFppj\nPredicted: IUHFppl\n\nEpoch 32 complete, Average Loss: 1.3553\nAccuracy: 0.0300\nSample 112\nOriginal text: Invde\nPredicted: Tnvde\n\nEpoch 33 complete, Average Loss: 1.3157\nAccuracy: 0.0275\nSample 309\nOriginal text: uBAn\nPredicted: uBKn\n\nEpoch 34 complete, Average Loss: 1.2950\nAccuracy: 0.0700\nSample 291\nOriginal text: Iaxy\nPredicted: Iaxy\n\nEpoch 35 complete, Average Loss: 1.2192\nAccuracy: 0.0475\nSample 271\nOriginal text: PyGKJi\nPredicted: OvgKJi\n\nEpoch 36 complete, Average Loss: 1.2411\nAccuracy: 0.0550\nSample 284\nOriginal text: Tkxbdn\nPredicted: Wbbdm\n\nEpoch 37 complete, Average Loss: 1.1351\nAccuracy: 0.0050\nSample 228\nOriginal text: Aedp\nPredicted: Zedp\n\nEpoch 38 complete, Average Loss: 1.2183\nAccuracy: 0.0550\nSample 45\nOriginal text: Jwdbdy\nPredicted: Nwdbdy\n\nEpoch 39 complete, Average Loss: 1.1450\nAccuracy: 0.0225\nSample 162\nOriginal text: SfSxft\nPredicted: fzxff\n\nEpoch 40 complete, Average Loss: 1.1197\nAccuracy: 0.0000\nSample 85\nOriginal text: lgaQbnZ\nPredicted: IgaGbnX\n\nEpoch 41 complete, Average Loss: 1.1368\nAccuracy: 0.0375\nSample 215\nOriginal text: Utxpif\nPredicted: vpjf\n\nEpoch 42 complete, Average Loss: 1.1041\nAccuracy: 0.0725\nSample 199\nOriginal text: hmARZ\nPredicted: hmNRZ\n\nEpoch 43 complete, Average Loss: 0.9438\nAccuracy: 0.0175\nSample 150\nOriginal text: vUly\nPredicted: xOk\n\nEpoch 44 complete, Average Loss: 0.9010\nAccuracy: 0.2175\nSample 274\nOriginal text: Dbpb\nPredicted: Dbpb\n\nEpoch 45 complete, Average Loss: 0.8726\nAccuracy: 0.2075\nSample 50\nOriginal text: tuREb\nPredicted: IwREbk\n\nEpoch 46 complete, Average Loss: 0.7674\nAccuracy: 0.3275\nSample 277\nOriginal text: pDpF\nPredicted: pRpF\n\nEpoch 47 complete, Average Loss: 0.7381\nAccuracy: 0.2650\nSample 183\nOriginal text: Cqwkuy\nPredicted: VPclYy\n\nEpoch 48 complete, Average Loss: 0.7752\nAccuracy: 0.1300\nSample 231\nOriginal text: NDtnVd\nPredicted: NOknd\n\nEpoch 49 complete, Average Loss: 0.6885\nAccuracy: 0.3950\nSample 395\nOriginal text: Zmxvum\nPredicted: Zmxvum\n\nEpoch 50 complete, Average Loss: 0.7091\nAccuracy: 0.3025\nSample 142\nOriginal text: Tvjch\nPredicted: Tvjch\n\nEpoch 51 complete, Average Loss: 0.6250\nAccuracy: 0.3650\nSample 172\nOriginal text: Txou\nPredicted: Txou\n\nEpoch 52 complete, Average Loss: 0.5929\nAccuracy: 0.4275\nSample 107\nOriginal text: yHAj\nPredicted: yTAj\n\nEpoch 53 complete, Average Loss: 0.6324\nAccuracy: 0.3925\nSample 31\nOriginal text: aGAR\nPredicted: aGAE\n\nEpoch 54 complete, Average Loss: 0.5774\nAccuracy: 0.4200\nSample 151\nOriginal text: vUly\nPredicted: vUb\n\nEpoch 55 complete, Average Loss: 0.5539\nAccuracy: 0.4900\nSample 193\nOriginal text: Vlqggnb\nPredicted: Vlqggnb\n\nEpoch 56 complete, Average Loss: 0.5521\nAccuracy: 0.4125\nSample 201\nOriginal text: Qanqi\nPredicted: vQanqlv\n\nEpoch 57 complete, Average Loss: 0.5732\nAccuracy: 0.0000\nSample 204\nOriginal text: Psszbn\nPredicted: Psszbn\n\nEpoch 58 complete, Average Loss: 0.5534\nAccuracy: 0.4750\nSample 134\nOriginal text: mUoyZdw\nPredicted: nUoYZdw\n\nEpoch 59 complete, Average Loss: 0.6120\nAccuracy: 0.3725\nSample 307\nOriginal text: WcLP\nPredicted: WxLD\n\nEpoch 60 complete, Average Loss: 0.5516\nAccuracy: 0.4150\nSample 283\nOriginal text: gJeoZjL\nPredicted: oJeoZJk\n\nEpoch 61 complete, Average Loss: 0.5454\nAccuracy: 0.4450\nSample 20\nOriginal text: cOtHS\nPredicted: cDvDZ\n\nEpoch 62 complete, Average Loss: 0.5260\nAccuracy: 0.4775\nSample 79\nOriginal text: Dxuy\nPredicted: Dxuy\n\nEpoch 63 complete, Average Loss: 0.4831\nAccuracy: 0.4750\nSample 148\nOriginal text: Hdxeqg\nPredicted: Hdxeqg\n\nEpoch 64 complete, Average Loss: 0.4828\nAccuracy: 0.4400\nSample 142\nOriginal text: zfEy\nPredicted: zfEy\n\nEpoch 65 complete, Average Loss: 0.4933\nAccuracy: 0.4675\nSample 327\nOriginal text: ekCqU\nPredicted: ekGqU\n\nEpoch 66 complete, Average Loss: 0.4530\nAccuracy: 0.4450\nSample 287\nOriginal text: oUtQY\nPredicted: aUTGY\n\nEpoch 67 complete, Average Loss: 0.4907\nAccuracy: 0.4850\nSample 142\nOriginal text: Mexp\nPredicted: Mexp\n\nEpoch 68 complete, Average Loss: 0.4321\nAccuracy: 0.5000\nSample 80\nOriginal text: oUtQY\nPredicted: aUtGY\n\nEpoch 69 complete, Average Loss: 0.4545\nAccuracy: 0.4425\nSample 80\nOriginal text: Ebijl\nPredicted: Ebijl\n\nEpoch 70 complete, Average Loss: 0.4180\nAccuracy: 0.5025\nSample 95\nOriginal text: BXjdDb\nPredicted: BXjadDb\n\nEpoch 71 complete, Average Loss: 0.3929\nAccuracy: 0.4375\nSample 324\nOriginal text: YUWil\nPredicted: yUwfi\n\nEpoch 72 complete, Average Loss: 0.4701\nAccuracy: 0.4175\nSample 209\nOriginal text: ekCqU\nPredicted: ekCqU\n\nEpoch 73 complete, Average Loss: 0.4941\nAccuracy: 0.3725\nSample 73\nOriginal text: Yynzdy\nPredicted: Yynzdy\n\nEpoch 74 complete, Average Loss: 0.4340\nAccuracy: 0.4700\nSample 111\nOriginal text: ncRoW\nPredicted: ncRoW\n\nEpoch 75 complete, Average Loss: 0.4187\nAccuracy: 0.4650\nSample 239\nOriginal text: YsFPQYE\nPredicted: YzFPQYE\n\nEpoch 76 complete, Average Loss: 0.3957\nAccuracy: 0.4575\nSample 75\nOriginal text: mqYRn\nPredicted: mqYRm\n\nEpoch 77 complete, Average Loss: 0.4067\nAccuracy: 0.4875\nSample 360\nOriginal text: fmzhcxA\nPredicted: FmzhcxA\n\nEpoch 78 complete, Average Loss: 0.3904\nAccuracy: 0.5075\nSample 360\nOriginal text: MlwODm\nPredicted: MlvODm\n\nEpoch 79 complete, Average Loss: 0.4132\nAccuracy: 0.4400\nSample 50\nOriginal text: Bcspo\nPredicted: Bcspo\n\nEpoch 80 complete, Average Loss: 0.3970\nAccuracy: 0.4650\nSample 22\nOriginal text: InYvtI\nPredicted: lnYvtl\n\nEpoch 81 complete, Average Loss: 0.3506\nAccuracy: 0.5350\nSample 387\nOriginal text: Ofbq\nPredicted: Ofbq\n\nEpoch 82 complete, Average Loss: 0.3577\nAccuracy: 0.3775\nSample 63\nOriginal text: Jnjpr\nPredicted: Jmjpr\n\nEpoch 83 complete, Average Loss: 0.4221\nAccuracy: 0.4600\nSample 84\nOriginal text: Bxfwc\nPredicted: Bxfwc\n\nEpoch 84 complete, Average Loss: 0.3963\nAccuracy: 0.4775\nSample 284\nOriginal text: Wlrlzi\nPredicted: Wlrizt\n\nEpoch 85 complete, Average Loss: 0.4265\nAccuracy: 0.4875\nSample 21\nOriginal text: YsFPQYE\nPredicted: YsFFOYE\n\nEpoch 86 complete, Average Loss: 0.3760\nAccuracy: 0.3725\nSample 194\nOriginal text: Krnwou\nPredicted: Krnwou\n\nEpoch 87 complete, Average Loss: 0.4610\nAccuracy: 0.4025\nSample 281\nOriginal text: Cqwkuy\nPredicted: COoRy\n\nEpoch 88 complete, Average Loss: 0.4532\nAccuracy: 0.1250\nSample 96\nOriginal text: Bxcq\nPredicted: Bxcq\n\nEpoch 89 complete, Average Loss: 0.4378\nAccuracy: 0.4875\nSample 190\nOriginal text: Pndj\nPredicted: Pmdj\n\nEpoch 90 complete, Average Loss: 0.4498\nAccuracy: 0.4375\nSample 365\nOriginal text: Xohmb\nPredicted: Xohmb\n\nEpoch 91 complete, Average Loss: 0.4378\nAccuracy: 0.5100\nSample 386\nOriginal text: Sfmdtj\nPredicted: Stmdtj\n\nEpoch 92 complete, Average Loss: 0.3779\nAccuracy: 0.5000\nSample 339\nOriginal text: Pndj\nPredicted: Pndj\n\nEpoch 93 complete, Average Loss: 0.3741\nAccuracy: 0.5250\nSample 76\nOriginal text: Fbkb\nPredicted: Fbkb\n\nEpoch 94 complete, Average Loss: 0.3711\nAccuracy: 0.2675\nSample 156\nOriginal text: xMzE\nPredicted: xWZE\n\nEpoch 95 complete, Average Loss: 0.3903\nAccuracy: 0.4800\nSample 4\nOriginal text: Erumoct\nPredicted: Erumoct\n\nEpoch 96 complete, Average Loss: 0.4002\nAccuracy: 0.4975\nSample 73\nOriginal text: wkNRq\nPredicted: wkNCq\n\nEpoch 97 complete, Average Loss: 0.3730\nAccuracy: 0.5600\nSample 71\nOriginal text: Feunyj\nPredicted: Feunyj\n\nEpoch 98 complete, Average Loss: 0.3450\nAccuracy: 0.4325\nSample 166\nOriginal text: Dxuy\nPredicted: Dxuy\n\nEpoch 99 complete, Average Loss: 0.3174\nAccuracy: 0.5800\nSample 150\nOriginal text: mUoyZdw\nPredicted: mUohZdw\n\nEpoch 100 complete, Average Loss: 0.3182\nAccuracy: 0.5825\n","output_type":"stream"}],"execution_count":26}]}