{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10567348,"sourceType":"datasetVersion","datasetId":6539013}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, random_split\nimport os\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndata_dir = '/kaggle/input/task-1-data/data'\nimage_size = 128\ndata_transform = transforms.Compose([\n    transforms.Resize([image_size, image_size]),\n    transforms.Grayscale(num_output_channels=1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\n\nfull_dataset = datasets.ImageFolder(root=data_dir, transform=data_transform)\n\ntrain_size = int(0.8* len(full_dataset))\nval_size = (len(full_dataset) - train_size)//2\ntest_size = len(full_dataset) - train_size -  val_size\n\ntrain_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader  = DataLoader(val_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32)\n\n\nclass Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=32, padding=1, kernel_size=3),\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.MaxPool2d(kernel_size=2),\n            \n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(kernel_size=2),\n            \n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.MaxPool2d(kernel_size=2),\n        )\n        \n        self.dense_layers = nn.Sequential(\n            nn.Flatten(),\n            nn.Dropout(0.3),\n            nn.Linear(128 * 16 * 16, 256),\n            nn.ReLU(),\n            nn.BatchNorm1d(256),\n            nn.Dropout(0.3),\n            nn.Linear(256, 100)\n        )\n    \n    def forward(self,x):\n        x =self.features(x)\n        x = self.dense_layers(x)\n        return x\n    \nmodel = Classifier().to(device)\n\nloss_func = nn.CrossEntropyLoss()\noptimiser = optim.Adam(model.parameters(), lr=0.0003)\n\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0\n    correct = 0\n    total = 0\n    \n    for images, labels in train_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        outputs = model(images)\n        loss = loss_func(outputs, labels)\n        \n        optimiser.zero_grad()\n        loss.backward()\n        optimiser.step()\n        \n        running_loss += loss.item()\n        max_val, prediction = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (prediction==labels).sum().item()\n        \n    train_loss = running_loss/len(train_loader)\n    train_accuracy = (correct/total)*100\n    \n    \n    model.eval()\n    val_loss = 0\n    correct = 0\n    total = 0\n    \n    for images, labels in val_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        outputs = model(images)\n        loss = loss_func(outputs, labels)\n        \n        val_loss += loss.item()\n        max_val, prediction = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (prediction == labels).sum().item()\n        \n    val_loss = val_loss / len(val_loader)\n    val_acc = 100 * correct / total\n        \n    print(f'Epoch [{epoch+1}/{num_epochs}]')\n    print(f'Train Loss: {train_loss:.4f}, Acc: {train_accuracy:.2f}%')\n    print(f'Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-25T14:21:01.523670Z","iopub.execute_input":"2025-01-25T14:21:01.523996Z","iopub.status.idle":"2025-01-25T14:23:37.707357Z","shell.execute_reply.started":"2025-01-25T14:21:01.523971Z","shell.execute_reply":"2025-01-25T14:23:37.706570Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/10]\nTrain Loss: 4.2202, Acc: 8.01%\nVal Loss: 3.6517, Acc: 22.27%\nEpoch [2/10]\nTrain Loss: 2.9709, Acc: 53.41%\nVal Loss: 2.6508, Acc: 63.18%\nEpoch [3/10]\nTrain Loss: 1.7254, Acc: 91.22%\nVal Loss: 2.0025, Acc: 78.64%\nEpoch [4/10]\nTrain Loss: 0.8858, Acc: 98.92%\nVal Loss: 1.7800, Acc: 81.36%\nEpoch [5/10]\nTrain Loss: 0.4476, Acc: 99.91%\nVal Loss: 1.4750, Acc: 85.23%\nEpoch [6/10]\nTrain Loss: 0.2139, Acc: 100.00%\nVal Loss: 1.3199, Acc: 88.64%\nEpoch [7/10]\nTrain Loss: 0.1211, Acc: 100.00%\nVal Loss: 1.2815, Acc: 86.59%\nEpoch [8/10]\nTrain Loss: 0.0776, Acc: 100.00%\nVal Loss: 1.2350, Acc: 87.27%\nEpoch [9/10]\nTrain Loss: 0.0578, Acc: 100.00%\nVal Loss: 1.1301, Acc: 89.09%\nEpoch [10/10]\nTrain Loss: 0.0402, Acc: 100.00%\nVal Loss: 1.0762, Acc: 88.64%\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"model.eval()\ntest_loss = 0\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        outputs = model(images)\n        loss = loss_func(outputs, labels)\n        \n        test_loss += loss.item()\n        max_val, prediction = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (prediction == labels).sum().item()\n        \ntest_loss = test_loss / len(test_loader)\ntest_acc = 100 * correct / total\n\nprint(f'\\nTest Results:')\nprint(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T14:24:47.452354Z","iopub.execute_input":"2025-01-25T14:24:47.452657Z","iopub.status.idle":"2025-01-25T14:24:50.602886Z","shell.execute_reply.started":"2025-01-25T14:24:47.452635Z","shell.execute_reply":"2025-01-25T14:24:50.602162Z"}},"outputs":[{"name":"stdout","text":"\nTest Results:\nTest Loss: 1.0911, Test Accuracy: 88.18%\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}